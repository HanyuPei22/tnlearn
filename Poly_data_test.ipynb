{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Polynomial to Construct Neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Poly_tensor_regressor import PolynomialTensorRegression\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from seeds import random_seed\n",
    "from neurons_better import Neurons\n",
    "import torch\n",
    "from mydata import MyData\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ny_torch import ny2tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 2.1893 + 0.6835 @ x**1 - 0.4742 @ x**2 + 0.6720 @ x**3\n"
     ]
    }
   ],
   "source": [
    "ori_data = datasets.fetch_california_housing()  # 加州房价数据——(20640, 8)\n",
    "x_data = ori_data.data\n",
    "y_data = ori_data.target.reshape(-1, 1)\n",
    "decomp_rank = 3\n",
    "poly_order = 4\n",
    "\n",
    "neuron = PolynomialTensorRegression(decomp_rank, \n",
    "                                    poly_order, \n",
    "                                    method='cp', \n",
    "                                    reg_lambda_w=0.01, \n",
    "                                    reg_lambda_c=0.01) \n",
    "\n",
    "neuron.fit(x_data,y_data)\n",
    "print(neuron.neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run times: 1\n",
      "epoch:  100  net_1_val_error:  0.07589223\n",
      "epoch:  200  net_1_val_error:  0.07318521\n",
      "epoch:  300  net_1_val_error:  0.07165239\n",
      "net_1_test_error:  0.07525507 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07415062\n",
      "epoch:  200  net_2_val_error:  0.07245491\n",
      "epoch:  300  net_2_val_error:  0.07019166\n",
      "net_2_test_error:  0.07178815 \n",
      "\n",
      "run times: 2\n",
      "epoch:  100  net_1_val_error:  0.07690656\n",
      "epoch:  200  net_1_val_error:  0.07268754\n",
      "epoch:  300  net_1_val_error:  0.069136046\n",
      "net_1_test_error:  0.07420971 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07141983\n",
      "epoch:  200  net_2_val_error:  0.06734463\n",
      "epoch:  300  net_2_val_error:  0.06568623\n",
      "net_2_test_error:  0.07058753 \n",
      "\n",
      "run times: 3\n",
      "epoch:  100  net_1_val_error:  0.080942765\n",
      "epoch:  200  net_1_val_error:  0.07413984\n",
      "epoch:  300  net_1_val_error:  0.07329793\n",
      "net_1_test_error:  0.07251012 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07620826\n",
      "epoch:  200  net_2_val_error:  0.07153936\n",
      "epoch:  300  net_2_val_error:  0.07115678\n",
      "net_2_test_error:  0.06974656 \n",
      "\n",
      "run times: 4\n",
      "epoch:  100  net_1_val_error:  0.080301754\n",
      "epoch:  200  net_1_val_error:  0.07381877\n",
      "epoch:  300  net_1_val_error:  0.06837532\n",
      "net_1_test_error:  0.068316735 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07599251\n",
      "epoch:  200  net_2_val_error:  0.07143177\n",
      "epoch:  300  net_2_val_error:  0.07271665\n",
      "net_2_test_error:  0.070865355 \n",
      "\n",
      "run times: 5\n",
      "epoch:  100  net_1_val_error:  0.08374981\n",
      "epoch:  200  net_1_val_error:  0.08348475\n",
      "epoch:  300  net_1_val_error:  0.0759615\n",
      "net_1_test_error:  0.07585073 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.08164927\n",
      "epoch:  200  net_2_val_error:  0.08263191\n",
      "epoch:  300  net_2_val_error:  0.07226783\n",
      "net_2_test_error:  0.07237603 \n",
      "\n",
      "run times: 6\n",
      "epoch:  100  net_1_val_error:  0.078789294\n",
      "epoch:  200  net_1_val_error:  0.073989056\n",
      "epoch:  300  net_1_val_error:  0.0724086\n",
      "net_1_test_error:  0.07018228 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07530509\n",
      "epoch:  200  net_2_val_error:  0.07424783\n",
      "epoch:  300  net_2_val_error:  0.07083888\n",
      "net_2_test_error:  0.06804919 \n",
      "\n",
      "run times: 7\n",
      "epoch:  100  net_1_val_error:  0.074706055\n",
      "epoch:  200  net_1_val_error:  0.07167414\n",
      "epoch:  300  net_1_val_error:  0.06970886\n",
      "net_1_test_error:  0.07267311 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.075555146\n",
      "epoch:  200  net_2_val_error:  0.06911098\n",
      "epoch:  300  net_2_val_error:  0.066544086\n",
      "net_2_test_error:  0.06938232 \n",
      "\n",
      "run times: 8\n",
      "epoch:  100  net_1_val_error:  0.07539472\n",
      "epoch:  200  net_1_val_error:  0.0709233\n",
      "epoch:  300  net_1_val_error:  0.06913649\n",
      "net_1_test_error:  0.07093361 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07413153\n",
      "epoch:  200  net_2_val_error:  0.07271481\n",
      "epoch:  300  net_2_val_error:  0.069433786\n",
      "net_2_test_error:  0.07052714 \n",
      "\n",
      "run times: 9\n",
      "epoch:  100  net_1_val_error:  0.07868718\n",
      "epoch:  200  net_1_val_error:  0.0717256\n",
      "epoch:  300  net_1_val_error:  0.06936159\n",
      "net_1_test_error:  0.07126218 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07537837\n",
      "epoch:  200  net_2_val_error:  0.06979953\n",
      "epoch:  300  net_2_val_error:  0.06749879\n",
      "net_2_test_error:  0.070127405 \n",
      "\n",
      "run times: 10\n",
      "epoch:  100  net_1_val_error:  0.078528814\n",
      "epoch:  200  net_1_val_error:  0.077084005\n",
      "epoch:  300  net_1_val_error:  0.07142924\n",
      "net_1_test_error:  0.06927248 \n",
      "\n",
      "epoch:  100  net_2_val_error:  0.07622154\n",
      "epoch:  200  net_2_val_error:  0.07152518\n",
      "epoch:  300  net_2_val_error:  0.07182365\n",
      "net_2_test_error:  0.067651786 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(seed):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ori_data = datasets.fetch_california_housing()  # 加州房价数据——(20640, 8)\n",
    "    x_data = ori_data.data\n",
    "    y_data = ori_data.target.reshape(-1, 1)\n",
    "\n",
    "    train_x, x_test, train_y, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=seed)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.15, random_state=seed)\n",
    "\n",
    "    scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "    x_train = ny2tensor(scaler_x.fit_transform(x_train))\n",
    "    x_valid = ny2tensor(scaler_x.transform(x_valid))\n",
    "    x_test = ny2tensor(scaler_x.transform(x_test))\n",
    "\n",
    "    scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "    y_train = ny2tensor(scaler_y.fit_transform(y_train))\n",
    "    y_valid = ny2tensor(scaler_y.transform(y_valid))\n",
    "    y_test = ny2tensor(scaler_y.transform(y_test))\n",
    "\n",
    "    max_epoch = 300\n",
    "    batch_size = 128\n",
    "\n",
    "    trainset = MyData(x_train, y_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # =======================================================================================================\n",
    "\n",
    "    random_seed(seed)\n",
    "    sr1 = '0.06@x**3 + 0.14@x + 0.76'\n",
    "\n",
    "    net1 = nn.Sequential(Neurons(8, 6, sr1),\n",
    "                         nn.Sigmoid(),\n",
    "\n",
    "                         Neurons(6, 3, sr1),\n",
    "                         nn.Sigmoid(),\n",
    "\n",
    "                         Neurons(3, 1, sr1),\n",
    "                         ).to(device)\n",
    "\n",
    "    cost1 = nn.MSELoss().to(device)\n",
    "    optimizer1 = torch.optim.RMSprop(net1.parameters(), lr=0.001)\n",
    "\n",
    "    initial_score1 = float(\"inf\")\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        for inputs1, labels1 in trainloader:\n",
    "            inputs1 = inputs1.to(device)\n",
    "            labels1 = labels1.to(device)\n",
    "            predict1 = net1(inputs1)\n",
    "            loss1 = cost1(predict1, labels1)\n",
    "            optimizer1.zero_grad()\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            error1 = mean_squared_error(net1(x_valid.to(device)).cpu(), y_valid)\n",
    "            if error1 < initial_score1:\n",
    "                initial_score1 = error1\n",
    "                model_best1 = net1\n",
    "\n",
    "        if (k+1) % 100 == 0:\n",
    "            print('epoch: ', k + 1, ' net_1_val_error: ', error1)\n",
    "\n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        test_error1 = mean_squared_error(model_best1(x_test.to(device)).cpu(), y_test)\n",
    "\n",
    "    print('net_1_test_error: ', test_error1, '\\n')\n",
    "\n",
    "# =================================================================================================\n",
    "# =================================================================================================\n",
    "\n",
    "    random_seed(seed)\n",
    "    sr2 = '- 2.1893 + 0.6835 @ x**1 - 0.4742 @ x**2 + 0.6720 @ x**3'\n",
    "\n",
    "    net2 = nn.Sequential(Neurons(8, 6, sr2),\n",
    "                         nn.Sigmoid(),\n",
    "\n",
    "                         Neurons(6, 3, sr2),\n",
    "                         nn.Sigmoid(),\n",
    "\n",
    "                         Neurons(3, 1, sr2),\n",
    "                         ).to(device)\n",
    "\n",
    "    cost2 = nn.MSELoss().to(device)\n",
    "    optimizer2 = torch.optim.RMSprop(net2.parameters(), lr=0.001)\n",
    "\n",
    "    initial_score2 = float(\"inf\")\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        for inputs2, labels2 in trainloader:\n",
    "            inputs2 = inputs2.to(device)\n",
    "            labels2 = labels2.to(device)\n",
    "            predict2 = net2(inputs2)\n",
    "            loss2 = cost2(predict2, labels2)\n",
    "            optimizer2.zero_grad()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            error2 = mean_squared_error(net2(x_valid.to(device)).cpu(), y_valid)\n",
    "            if error2 < initial_score2:\n",
    "                initial_score2 = error2\n",
    "                model_best2 = net2\n",
    "\n",
    "        if (k+1) % 100 == 0:\n",
    "            print('epoch: ', k + 1, ' net_2_val_error: ', error2)\n",
    "\n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        test_error2 = mean_squared_error(model_best2(x_test.to(device)).cpu(), y_test)\n",
    "\n",
    "    print('net_2_test_error: ', test_error2, '\\n')\n",
    "\n",
    "    return test_error1, test_error2\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    error1_list = []\n",
    "    error2_list = []\n",
    "    sv = np.empty((1, 4))\n",
    "\n",
    "    for i in range(10):\n",
    "        print('run times:', i + 1)\n",
    "        error_1, error_2 = main(i + 1)\n",
    "\n",
    "        error1_list.append(error_1)\n",
    "        error2_list.append(error_2)\n",
    "\n",
    "    sv[0, 0] = np.mean(error1_list)\n",
    "    sv[0, 1] = np.std(error1_list)\n",
    "\n",
    "    sv[0, 2] = np.mean(error2_list)\n",
    "    sv[0, 3] = np.std(error2_list)\n",
    "\n",
    "    np.savetxt('california_housing_' + 'result.csv', sv, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = datasets.fetch_openml('house_sales', version = 8)  # 房价数据——(21613, 15)\n",
    "x_data = np.array(ori_data.data, dtype=np.float32)\n",
    "y_data = np.array(ori_data.target, dtype=np.float32).reshape((-1, 1))\n",
    "decomp_rank = 3\n",
    "poly_order = 4\n",
    "\n",
    "neuron = PolynomialTensorRegression(decomp_rank, \n",
    "                                    poly_order, \n",
    "                                    method='cp', \n",
    "                                    reg_lambda_w=0.01, \n",
    "                                    reg_lambda_c=0.01) \n",
    "\n",
    "neuron.fit(x_data,y_data) \n",
    "print(neuron.neuron)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
